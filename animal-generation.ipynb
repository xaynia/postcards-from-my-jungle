{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8ewBwmjWUgj"
      },
      "outputs": [],
      "source": [
        "# Install (run once per fresh runtime)\n",
        "!pip -q install diffusers datasets transformers accelerate scipy ftfy\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import datasets\n",
        "import diffusers\n",
        "import huggingface_hub\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from torch.nn import functional as F\n",
        "from google.colab import drive\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Faster GPU math\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Suppress warnings\n",
        "datasets.logging.set_verbosity_error()\n",
        "diffusers.logging.set_verbosity_error()\n",
        "huggingface_hub.logging.set_verbosity_error()\n",
        "\n",
        "# Mount Drive for persistent saving\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "drive_ckpt = \"/content/drive/MyDrive/a5-animal_latest.pth\"\n",
        "\n",
        "# Dataset (keep small for speed, increase later if you have time)\n",
        "dataset = load_dataset(\"ldgravy/Medieval-Bestiary\", split=\"train\")\n",
        "dataset = dataset.select(range(512))  # faster than 1024\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "\n",
        "# Preprocess\n",
        "image_size = 64\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def transform_batch(examples):\n",
        "    examples[\"pixel_values\"] = [preprocess(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "    return examples\n",
        "\n",
        "dataset.set_transform(transform_batch)\n",
        "\n",
        "class TensorDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset):\n",
        "        self.hf_dataset = hf_dataset\n",
        "    def __len__(self):\n",
        "        return len(self.hf_dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.hf_dataset[idx][\"pixel_values\"]\n",
        "\n",
        "tensor_dataset = TensorDataset(dataset)\n",
        "\n",
        "# DataLoader tuning\n",
        "batch_size = 64 if device == \"cuda\" else 16  # bump if stable, lower if OOM\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    tensor_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=(device == \"cuda\"),\n",
        "    persistent_workers=True,\n",
        ")\n",
        "\n",
        "# Model\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 256, 256, 512),\n",
        "    down_block_types=(\"DownBlock2D\",\"DownBlock2D\",\"AttnDownBlock2D\",\"AttnDownBlock2D\"),\n",
        "    up_block_types=(\"AttnUpBlock2D\",\"AttnUpBlock2D\",\"UpBlock2D\",\"UpBlock2D\"),\n",
        ").to(device)\n",
        "\n",
        "# Resume if checkpoint exists\n",
        "start_epoch = 0\n",
        "if os.path.exists(drive_ckpt):\n",
        "    model.load_state_dict(torch.load(drive_ckpt, map_location=device))\n",
        "    print(\"Resumed from:\", drive_ckpt)\n",
        "else:\n",
        "    print(\"No checkpoint found, training from scratch.\")\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Mixed precision (GPU only) - big speedup\n",
        "use_amp = (device == \"cuda\")\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "# Train (AMP enabled if on GPU)\n",
        "num_epochs = 15\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        clean_images = batch.to(device, non_blocking=True)\n",
        "        noise = torch.randn_like(clean_images)\n",
        "        timesteps = torch.randint(\n",
        "            0,\n",
        "            noise_scheduler.config.num_train_timesteps,\n",
        "            (clean_images.size(0),),\n",
        "            device=device,\n",
        "        ).long()\n",
        "\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            noise_pred = model(noisy_images, timesteps).sample\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Epoch {epoch} Step {step} Loss {loss.item():.4f}\")\n",
        "\n",
        "    # Save every epoch to Drive\n",
        "    torch.save(model.state_dict(), drive_ckpt)\n",
        "    print(\"Saved to Drive:\", drive_ckpt)\n",
        "\n",
        "print(\"Done. Final checkpoint:\", drive_ckpt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra training (needed based on test renders)"
      ],
      "metadata": {
        "id": "vf3puitQz-9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training more epochs\n",
        "\n",
        "extra_epochs = 50\n",
        "\n",
        "for e in range(extra_epochs):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        clean_images = batch.to(device, non_blocking=True)\n",
        "        noise = torch.randn_like(clean_images)\n",
        "        timesteps = torch.randint(\n",
        "            0,\n",
        "            noise_scheduler.config.num_train_timesteps,\n",
        "            (clean_images.size(0),),\n",
        "            device=device,\n",
        "        ).long()\n",
        "\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        noise_pred = model(noisy_images, timesteps).sample\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Extra epoch {e} Step {step} Loss {loss.item():.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), drive_ckpt)\n",
        "    print(\"Saved to Drive:\", drive_ckpt)\n",
        "\n",
        "print(\"Extra training done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq2bd57GGiu2",
        "outputId": "e7569c19-a474-4b90-8c19-d9ff4e6b1393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extra epoch 0 Step 0 Loss 0.0397\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 1 Step 0 Loss 0.0412\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 2 Step 0 Loss 0.0649\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 3 Step 0 Loss 0.0521\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 4 Step 0 Loss 0.0354\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 5 Step 0 Loss 0.0696\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 6 Step 0 Loss 0.0580\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 7 Step 0 Loss 0.0470\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 8 Step 0 Loss 0.0373\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 9 Step 0 Loss 0.0684\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 10 Step 0 Loss 0.0617\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 11 Step 0 Loss 0.0627\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 12 Step 0 Loss 0.0451\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 13 Step 0 Loss 0.0502\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 14 Step 0 Loss 0.0450\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 15 Step 0 Loss 0.0695\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 16 Step 0 Loss 0.0652\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 17 Step 0 Loss 0.0598\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 18 Step 0 Loss 0.0515\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 19 Step 0 Loss 0.0429\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 20 Step 0 Loss 0.0563\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 21 Step 0 Loss 0.0662\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 22 Step 0 Loss 0.0449\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 23 Step 0 Loss 0.0488\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 24 Step 0 Loss 0.0391\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 25 Step 0 Loss 0.0516\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 26 Step 0 Loss 0.0491\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 27 Step 0 Loss 0.0518\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 28 Step 0 Loss 0.0416\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 29 Step 0 Loss 0.0520\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 30 Step 0 Loss 0.0563\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 31 Step 0 Loss 0.0469\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 32 Step 0 Loss 0.0395\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 33 Step 0 Loss 0.0687\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 34 Step 0 Loss 0.0371\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 35 Step 0 Loss 0.0640\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 36 Step 0 Loss 0.0404\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 37 Step 0 Loss 0.0391\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 38 Step 0 Loss 0.0654\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 39 Step 0 Loss 0.0502\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 40 Step 0 Loss 0.0464\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 41 Step 0 Loss 0.0504\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 42 Step 0 Loss 0.0384\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 43 Step 0 Loss 0.0762\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 44 Step 0 Loss 0.0633\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 45 Step 0 Loss 0.0697\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 46 Step 0 Loss 0.0538\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 47 Step 0 Loss 0.0548\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 48 Step 0 Loss 0.0418\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra epoch 49 Step 0 Loss 0.0297\n",
            "Saved to Drive: /content/drive/MyDrive/a5-animal_latest.pth\n",
            "Extra training done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training from Drive checkpoint with MORE updates (no restart-from-scratch)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "drive_ckpt = \"/content/drive/MyDrive/a5-animal_latest.pth\"\n",
        "assert os.path.exists(drive_ckpt), \"Checkpoint not found on Drive\"\n",
        "\n",
        "# Dataset (use more than 512)\n",
        "dataset = load_dataset(\"ldgravy/Medieval-Bestiary\", split=\"train\")\n",
        "dataset = dataset.select(range(1024))  # you can also use the full dataset (remove this line)\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "\n",
        "# Preprocess (same as before)\n",
        "image_size = 64\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def transform_batch(examples):\n",
        "    examples[\"pixel_values\"] = [preprocess(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "    return examples\n",
        "\n",
        "dataset.set_transform(transform_batch)\n",
        "\n",
        "class TensorDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset):\n",
        "        self.hf_dataset = hf_dataset\n",
        "    def __len__(self):\n",
        "        return len(self.hf_dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.hf_dataset[idx][\"pixel_values\"]\n",
        "\n",
        "tensor_dataset = TensorDataset(dataset)\n",
        "\n",
        "# Smaller batch size = more optimizer updates per epoch (key)\n",
        "batch_size = 16\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    tensor_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=(device == \"cuda\"),\n",
        ")\n",
        "\n",
        "# Model (same architecture)\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 256, 256, 512),\n",
        "    down_block_types=(\"DownBlock2D\",\"DownBlock2D\",\"AttnDownBlock2D\",\"AttnDownBlock2D\"),\n",
        "    up_block_types=(\"AttnUpBlock2D\",\"AttnUpBlock2D\",\"UpBlock2D\",\"UpBlock2D\"),\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(drive_ckpt, map_location=device))\n",
        "print(\"Loaded checkpoint:\", drive_ckpt)\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train by number of updates\n",
        "target_updates = 5000     # increase to 10000 if needed\n",
        "save_every = 500\n",
        "\n",
        "global_step = 0\n",
        "model.train()\n",
        "\n",
        "while global_step < target_updates:\n",
        "    for batch in train_dataloader:\n",
        "        clean_images = batch.to(device, non_blocking=True)\n",
        "        noise = torch.randn_like(clean_images)\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.config.num_train_timesteps,\n",
        "            (clean_images.size(0),),\n",
        "            device=device\n",
        "        ).long()\n",
        "\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        noise_pred = model(noisy_images, timesteps).sample\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % 100 == 0:\n",
        "            print(\"update\", global_step, \"loss\", float(loss.item()))\n",
        "\n",
        "        if global_step % save_every == 0:\n",
        "            torch.save(model.state_dict(), drive_ckpt)\n",
        "            print(\"Saved:\", drive_ckpt)\n",
        "\n",
        "        if global_step >= target_updates:\n",
        "            break\n",
        "\n",
        "torch.save(model.state_dict(), drive_ckpt)\n",
        "print(\"Done. Saved:\", drive_ckpt)\n"
      ],
      "metadata": {
        "id": "jmCtbcALXdzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training from Drive checkpoint with many more optimizer updates\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from google.colab import drive\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "drive_ckpt = \"/content/drive/MyDrive/a5-animal_latest.pth\"\n",
        "assert os.path.exists(drive_ckpt), \"Checkpoint not found: \" + drive_ckpt\n",
        "\n",
        "# Use more data\n",
        "dataset = load_dataset(\"ldgravy/Medieval-Bestiary\", split=\"train\")\n",
        "dataset = dataset.select(range(1024))\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "\n",
        "# Preprocess\n",
        "image_size = 64\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def transform_batch(examples):\n",
        "    examples[\"pixel_values\"] = [preprocess(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
        "    return examples\n",
        "\n",
        "dataset.set_transform(transform_batch)\n",
        "\n",
        "class TensorDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, hf_dataset):\n",
        "        self.hf_dataset = hf_dataset\n",
        "    def __len__(self):\n",
        "        return len(self.hf_dataset)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.hf_dataset[idx][\"pixel_values\"]\n",
        "\n",
        "tensor_dataset = TensorDataset(dataset)\n",
        "\n",
        "#  smaller batch size -> more steps -> more learning\n",
        "batch_size = 16\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    tensor_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=(device == \"cuda\"),\n",
        ")\n",
        "\n",
        "# Model (must match your architecture)\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 256, 256, 512),\n",
        "    down_block_types=(\"DownBlock2D\",\"DownBlock2D\",\"AttnDownBlock2D\",\"AttnDownBlock2D\"),\n",
        "    up_block_types=(\"AttnUpBlock2D\",\"AttnUpBlock2D\",\"UpBlock2D\",\"UpBlock2D\"),\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(drive_ckpt, map_location=device))\n",
        "print(\"Loaded checkpoint:\", drive_ckpt)\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Train by updates\n",
        "extra_updates = 10000\n",
        "save_every = 500\n",
        "\n",
        "global_step = 0\n",
        "model.train()\n",
        "\n",
        "while global_step < extra_updates:\n",
        "    for batch in train_dataloader:\n",
        "        clean_images = batch.to(device, non_blocking=True)\n",
        "        noise = torch.randn_like(clean_images)\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.config.num_train_timesteps,\n",
        "            (clean_images.size(0),),\n",
        "            device=device\n",
        "        ).long()\n",
        "\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        noise_pred = model(noisy_images, timesteps).sample\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % 100 == 0:\n",
        "            print(\"update\", global_step, \"loss\", float(loss.item()))\n",
        "\n",
        "        if global_step % save_every == 0:\n",
        "            torch.save(model.state_dict(), drive_ckpt)\n",
        "            print(\"Saved:\", drive_ckpt)\n",
        "\n",
        "        if global_step >= extra_updates:\n",
        "            break\n",
        "\n",
        "torch.save(model.state_dict(), drive_ckpt)\n",
        "print(\"Done. Saved:\", drive_ckpt)\n"
      ],
      "metadata": {
        "id": "CI5938HnkI9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated save images and zip: Switch to CPU due to rate-limits"
      ],
      "metadata": {
        "id": "KSCKoa000NE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 50 images from checkpoint and zip them\n",
        "\n",
        "import os, shutil, torch\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "from diffusers import UNet2DModel, DDPMPipeline, DDIMScheduler\n",
        "\n",
        "# --- device ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --- drive + ckpt ---\n",
        "drive.mount(\"/content/drive\")\n",
        "ckpt = \"/content/drive/MyDrive/a5-animal_latest.pth\"\n",
        "assert os.path.exists(ckpt), f\"Missing checkpoint: {ckpt}\"\n",
        "\n",
        "# --- model  ---\n",
        "image_size = 64\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 256, 256, 512),\n",
        "    down_block_types=(\"DownBlock2D\",\"DownBlock2D\",\"AttnDownBlock2D\",\"AttnDownBlock2D\"),\n",
        "    up_block_types=(\"AttnUpBlock2D\",\"AttnUpBlock2D\",\"UpBlock2D\",\"UpBlock2D\"),\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "model.eval()\n",
        "print(\"Loaded:\", ckpt)\n",
        "\n",
        "# --- pipeline: DDIM sampling (faster/cleaner than pure DDPM sampling) ---\n",
        "scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
        "pipe = DDPMPipeline(unet=model, scheduler=scheduler).to(device)\n",
        "\n",
        "out_dir = \"/content/animals_out\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "num_images = 50\n",
        "batch = 10\n",
        "steps = 200 if device==\"cuda\" else 50\n",
        "\n",
        "idx = 0\n",
        "while idx < num_images:\n",
        "    cur = min(batch, num_images - idx)\n",
        "    gens = [torch.Generator(device=device).manual_seed(1000 + idx + i) for i in range(cur)]\n",
        "    imgs = pipe(batch_size=cur, num_inference_steps=steps, generator=gens).images\n",
        "    for i, im in enumerate(imgs):\n",
        "        im.save(os.path.join(out_dir, f\"animal_{idx+i:03d}.png\"))\n",
        "    idx += cur\n",
        "    print(\"saved\", idx, \"/\", num_images)\n",
        "\n",
        "# zip + copy to drive\n",
        "zip_path = shutil.make_archive(\"/content/animals_out\", \"zip\", out_dir)\n",
        "drive_dest = \"/content/drive/MyDrive/animals_out.zip\"\n",
        "shutil.copy2(zip_path, drive_dest)\n",
        "\n",
        "print(\"ZIP saved to:\", drive_dest)\n"
      ],
      "metadata": {
        "id": "nGK9uZOItOwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cells below are unused, as I switched to CPU compiling"
      ],
      "metadata": {
        "id": "v62hgtU40cgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "aaNMT_fXnLt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of why I added extra training"
      ],
      "metadata": {
        "id": "3efYu4fD0pet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Test image generation (fixed seed, DDIM sampler, loads from Drive)\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "ckpt = \"/content/drive/MyDrive/a5-animal_latest.pth\"\n",
        "\n",
        "# Model\n",
        "image_size = 64\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 256, 256, 512),\n",
        "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\"),\n",
        "    up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(ckpt, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# DDIM scheduler for sampling\n",
        "scheduler = DDIMScheduler(num_train_timesteps=1000)\n",
        "pipe = DDPMPipeline(unet=model, scheduler=scheduler).to(device)\n",
        "\n",
        "# Fixed seed so results are comparable run to run\n",
        "g = torch.Generator(device=device).manual_seed(123)\n",
        "\n",
        "# Generate images\n",
        "images = pipe(batch_size=8, num_inference_steps=100, generator=g).images\n",
        "\n",
        "# Show grid\n",
        "plt.figure(figsize=(12, 3))\n",
        "for i, img in enumerate(images):\n",
        "    plt.subplot(1, 8, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LAwyGCfANE-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time\n",
        "ckpt = \"/content/drive/MyDrive/a5-animal_latest.pth\"\n",
        "print(\"exists:\", os.path.exists(ckpt))\n",
        "print(\"size:\", os.path.getsize(ckpt), \"bytes\")\n",
        "print(\"modified:\", time.ctime(os.path.getmtime(ckpt)))\n"
      ],
      "metadata": {
        "id": "SUNcouJwMHAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMScheduler\n",
        "\n",
        "# 1. Take one real image from the dataset (already transformed)\n",
        "sample = dataset[0][\"pixel_values\"].unsqueeze(0).to(device)\n",
        "\n",
        "# 2. Create a fresh scheduler just for this check (no set_timesteps here)\n",
        "check_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "\n",
        "# keep tensors on device\n",
        "t = torch.tensor([500], device=device, dtype=torch.long)\n",
        "\n",
        "# 4. Add noise to the real image\n",
        "noise = torch.randn_like(sample)\n",
        "noisy = check_scheduler.add_noise(sample, noise, t)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Predict noise\n",
        "    noise_pred = model(noisy, t).sample\n",
        "    # One reverse step: pass a CPU timestep to the scheduler\n",
        "    denoised = check_scheduler.step(noise_pred, t[0], noisy).prev_sample\n",
        "\n",
        "def denorm(x):\n",
        "    x = x[0].detach().cpu()\n",
        "    x = (x * 0.5) + 0.5  # [-1, 1] -> [0, 1]\n",
        "    x = torch.clamp(x, 0, 1)\n",
        "    return x.permute(1, 2, 0).numpy()\n",
        "\n",
        "orig_img = denorm(sample)\n",
        "noisy_img = denorm(noisy)\n",
        "denoised_img = denorm(denoised)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "axes[0].imshow(orig_img);     axes[0].set_title(\"Original\");          axes[0].axis(\"off\")\n",
        "axes[1].imshow(noisy_img);    axes[1].set_title(\"Noisy (t=500)\");     axes[1].axis(\"off\")\n",
        "axes[2].imshow(denoised_img); axes[2].set_title(\"One-step denoised\"); axes[2].axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xkgErMOyWXSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the length of the dataset\n",
        "print(f\"Length of the dataset: {len(dataset)}\")\n",
        "\n",
        "# Visualize some images from the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_images(dataset, num_images=5):\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "    for i in range(num_images):\n",
        "        # Get an image from the dataset\n",
        "        image = dataset[i][\"pixel_values\"]  # Access the preprocessed tensor\n",
        "        # Convert tensor to numpy and denormalize\n",
        "        image = image.permute(1, 2, 0).cpu().numpy()  # Change from (C, H, W) to (H, W, C)\n",
        "        image = (image * 0.5) + 0.5  # Denormalize from [-1, 1] to [0, 1]\n",
        "        axes[i].imshow(image)\n",
        "        axes[i].axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize 5 images\n",
        "visualize_images(dataset, num_images=8)"
      ],
      "metadata": {
        "id": "0isXxcGAWYTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "ckpt_path = \"/content/drive/MyDrive/a5-animal_latest.pth\"\n",
        "\n",
        "# Load the trained model\n",
        "model = UNet2DModel(\n",
        "    sample_size=64,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(128, 256, 256, 512),\n",
        "    down_block_types=(\"DownBlock2D\",\"DownBlock2D\",\"AttnDownBlock2D\",\"AttnDownBlock2D\"),\n",
        "    up_block_types=(\"AttnUpBlock2D\",\"AttnUpBlock2D\",\"UpBlock2D\",\"UpBlock2D\"),\n",
        ").to(device)\n",
        "\n",
        "# Load the saved model weights\n",
        "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Set up the noise scheduler\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise_scheduler.set_timesteps(100)\n",
        "\n",
        "def generate_images(num_images=1):\n",
        "    with torch.no_grad():\n",
        "        images = torch.randn((num_images, 3, 64, 64), device=device)\n",
        "        for t in noise_scheduler.timesteps:\n",
        "            model_input = noise_scheduler.scale_model_input(images, t)\n",
        "            noise_pred = model(model_input, t).sample\n",
        "            images = noise_scheduler.step(noise_pred, t, images).prev_sample\n",
        "\n",
        "        images = images.detach().cpu()\n",
        "        images = (images * 0.5) + 0.5\n",
        "        images = torch.clamp(images, 0, 1)\n",
        "        return images\n",
        "\n",
        "num_images = 5\n",
        "generated_images = generate_images(num_images)\n",
        "\n",
        "fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "for i, img in enumerate(generated_images):\n",
        "    axes[i].imshow(img.permute(1, 2, 0))\n",
        "    axes[i].axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SspB-Ee5WZbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create a directory to save the images\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "# Plot and save the generated images\n",
        "fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n",
        "for i, img in enumerate(generated_images):\n",
        "    img = img.permute(1, 2, 0)  # Change from (C, H, W) to (H, W, C)\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "    # Save each image with a zero-padded two-digit number\n",
        "    filename = f\"images/animal_{i:02d}.png\"\n",
        "    plt.imsave(filename, img.numpy())\n",
        "\n",
        "# Save the entire grid of images\n",
        "plt.savefig(\"images/animal_grid.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VFoFKw0eWaZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf images/\n",
        "\n",
        "# Utility to zip a file\n",
        "import shutil\n",
        "shutil.make_archive(\"animals\", \"zip\", \"images\")"
      ],
      "metadata": {
        "id": "jtaN-gSRWbMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "import shutil\n",
        "shutil.copy2(\"animals.zip\", \"/content/drive/MyDrive/animals.zip\")\n"
      ],
      "metadata": {
        "id": "tL8B22YNAwZG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}